{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Media Monitoring and Analysis\n",
    "\n",
    "Welcome! This notebook is geared to give you starter examples of how to use ChatGPT for social media monitoring and analysis. Specifically we will look at three use cases:\n",
    "- Sentiment and Emotion Analysis\n",
    "- Topic Extraction\n",
    "- Classification / Categorization\n",
    "\n",
    "For our purposes we will assume we work at Sam's Club (a well-known retail warehouse chain owned by Wal-Mart) and have been asked to look at a series of tweets for our analysis. I'll walk you through all the basic steps needed to do the analysis. \n",
    "\n",
    "NOTE: To make it easier to focus on the process of analysis instead of acquiring the data, I've included a JSON file that has curated output from Twitter (now called, X) in it. If you want to sign up for a developer account so you can get live data, go here: https://developer.twitter.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment and Emotion Analysis\n",
    "\n",
    "The first order of business is to take the data and put it into a format that is easier to analyze. Take a moment to look at the Tweets.json file and see how we get information from Twitter. Notice that the file is a list of dictionaries where each dictionary represents a tweet. We need to manipulate it into something usable. Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our packages \n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# set our key for openai to be used later\n",
    "openai_key = os.getenv('OPENAI_KEY')\n",
    "openai.api_key = openai_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Data\n",
    "\n",
    "Now that we have our packages ready to go and our OpenAI API key set we need to read the data info a pandas dataframe to ease our cleaning and analysis efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       created_at                   id               id_str  \\\n",
      "0  Mon May 15 18:21:47 +0000 2023  1658175862045302797  1658175862045302797   \n",
      "1  Mon May 15 17:29:18 +0000 2023  1658162654479888397  1658162654479888397   \n",
      "2  Mon May 15 17:04:13 +0000 2023  1658156342199332864  1658156342199332864   \n",
      "3  Mon May 15 16:54:52 +0000 2023  1658153988703809536  1658153988703809536   \n",
      "4  Mon May 15 16:54:24 +0000 2023  1658153870575521793  1658153870575521793   \n",
      "\n",
      "                                           full_text  truncated  \\\n",
      "0  KitchenAid Mixer Sale on @SamsClub | $90 Off P...      False   \n",
      "1           @KellieShai_ All we need is some milk. üç™      False   \n",
      "2  Happy National Chocolate Chip Day! No denying ...      False   \n",
      "3  This is heartbreaking that less than a year im...      False   \n",
      "4  @VIZIOsupport I bought a vizio tv from @SamsCl...      False   \n",
      "\n",
      "  display_text_range                                             source  \\\n",
      "0           [0, 128]  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
      "1           [13, 40]  <a href=\"https://prod1.sprinklr.com\" rel=\"nofo...   \n",
      "2           [0, 276]  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
      "3           [0, 168]  <a href=\"http://twitter.com/download/android\" ...   \n",
      "4           [0, 279]  <a href=\"http://twitter.com/download/android\" ...   \n",
      "\n",
      "   in_reply_to_status_id in_reply_to_status_id_str  in_reply_to_user_id  ...  \\\n",
      "0                    NaN                      None                  NaN  ...   \n",
      "1           1.658156e+18       1658156342199332864         1.629262e+18  ...   \n",
      "2                    NaN                      None                  NaN  ...   \n",
      "3           1.658154e+18       1658153870575521793         1.620974e+18  ...   \n",
      "4                    NaN                      None         6.915388e+07  ...   \n",
      "\n",
      "  place.contained_within place.bounding_box.type  \\\n",
      "0                    NaN                     NaN   \n",
      "1                    NaN                     NaN   \n",
      "2                    NaN                     NaN   \n",
      "3                    NaN                     NaN   \n",
      "4                    NaN                     NaN   \n",
      "\n",
      "   place.bounding_box.coordinates  geo.type  geo.coordinates coordinates.type  \\\n",
      "0                             NaN       NaN              NaN              NaN   \n",
      "1                             NaN       NaN              NaN              NaN   \n",
      "2                             NaN       NaN              NaN              NaN   \n",
      "3                             NaN       NaN              NaN              NaN   \n",
      "4                             NaN       NaN              NaN              NaN   \n",
      "\n",
      "   coordinates.coordinates  quoted_status.quoted_status_id  \\\n",
      "0                      NaN                             NaN   \n",
      "1                      NaN                             NaN   \n",
      "2                      NaN                             NaN   \n",
      "3                      NaN                             NaN   \n",
      "4                      NaN                             NaN   \n",
      "\n",
      "   quoted_status.quoted_status_id_str  quoted_status.possibly_sensitive  \n",
      "0                                 NaN                               NaN  \n",
      "1                                 NaN                               NaN  \n",
      "2                                 NaN                               NaN  \n",
      "3                                 NaN                               NaN  \n",
      "4                                 NaN                               NaN  \n",
      "\n",
      "[5 rows x 241 columns]\n"
     ]
    }
   ],
   "source": [
    "# Start by inspecting the json file and determining the structure of the data. Remember that the file is a list of dictionaries where each dictionary represents a tweet. \n",
    "# Some are nested dictionaries and some are not. We need to flatten the data so that we can convert it into a DataFrame.\n",
    "# The json_normalize function from pandas is used to convert JSON data into a flat table (DataFrame).\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Open the file named \"Tweets.json\" for reading. The \"with\" statement ensures that the file is properly closed after it is no longer needed.\n",
    "with open(\"Tweets.json\") as file:\n",
    "    # Use the json.load function to load the JSON data from the file into a Python object (usually a list or a dictionary).\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert the JSON data into a DataFrame. json_normalize flattens the data, meaning it can create a DataFrame from nested JSON data.\n",
    "df = json_normalize(data)\n",
    "\n",
    "# Print the first 5 rows of the DataFrame to see what the data looks like.\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame to a CSV file named \"initaldataframe.csv\" so you can see the progress. The argument index=False means that the DataFrame's index will not be saved in the CSV file.\n",
    "# While this step isn't necessary, it's a good idea to save your work as you go along and to check that the data looks correct.\n",
    "# If you are using VS Code, I highly recommend installing the Excel Viewer extension or Rainbow CSV extension so you can view the CSV file more easily. Or just open the file in Excel as well.\n",
    "df.to_csv('initaldataframe.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data Up\n",
    "\n",
    "We will do some very simple cleanup to make dealing with our data easier. First, let's take out columns that we obviously won't need for our goals to make the data even more easy to analyze. Take a look at the initaldataframe.csv and determine what columns you think should be kept for our analysis. At this early stage it's usually a good idea to keep a column if in doubt. We can always trim it out later. Since we are keeping it simple we can hack and slash a bit more than usual. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'lang', 'entities.hashtags', 'entities.symbols', 'entities.user_mentions', 'entities.urls', 'metadata.iso_language_code', 'metadata.result_type', 'user.id', 'user.id_str', 'user.name', 'user.screen_name', 'user.location', 'user.description', 'user.url', 'user.entities.url.urls', 'user.entities.description.urls', 'user.protected', 'user.followers_count', 'user.friends_count', 'user.listed_count', 'user.created_at', 'user.favourites_count', 'user.utc_offset', 'user.time_zone', 'user.geo_enabled', 'user.verified', 'user.statuses_count', 'user.lang', 'user.contributors_enabled', 'user.is_translator', 'user.is_translation_enabled', 'user.profile_background_color', 'user.profile_background_image_url', 'user.profile_background_image_url_https', 'user.profile_background_tile', 'user.profile_image_url', 'user.profile_image_url_https', 'user.profile_banner_url', 'user.profile_link_color', 'user.profile_sidebar_border_color', 'user.profile_sidebar_fill_color', 'user.profile_text_color', 'user.profile_use_background_image', 'user.has_extended_profile', 'user.default_profile', 'user.default_profile_image', 'user.following', 'user.follow_request_sent', 'user.notifications', 'user.translator_type', 'user.withheld_in_countries', 'entities.media', 'extended_entities.media', 'retweeted_status.created_at', 'retweeted_status.id', 'retweeted_status.id_str', 'retweeted_status.full_text', 'retweeted_status.truncated', 'retweeted_status.display_text_range', 'retweeted_status.entities.hashtags', 'retweeted_status.entities.symbols', 'retweeted_status.entities.user_mentions', 'retweeted_status.entities.urls', 'retweeted_status.entities.media', 'retweeted_status.extended_entities.media', 'retweeted_status.metadata.iso_language_code', 'retweeted_status.metadata.result_type', 'retweeted_status.source', 'retweeted_status.in_reply_to_status_id', 'retweeted_status.in_reply_to_status_id_str', 'retweeted_status.in_reply_to_user_id', 'retweeted_status.in_reply_to_user_id_str', 'retweeted_status.in_reply_to_screen_name', 'retweeted_status.user.id', 'retweeted_status.user.id_str', 'retweeted_status.user.name', 'retweeted_status.user.screen_name', 'retweeted_status.user.location', 'retweeted_status.user.description', 'retweeted_status.user.url', 'retweeted_status.user.entities.description.urls', 'retweeted_status.user.protected', 'retweeted_status.user.followers_count', 'retweeted_status.user.friends_count', 'retweeted_status.user.listed_count', 'retweeted_status.user.created_at', 'retweeted_status.user.favourites_count', 'retweeted_status.user.utc_offset', 'retweeted_status.user.time_zone', 'retweeted_status.user.geo_enabled', 'retweeted_status.user.verified', 'retweeted_status.user.statuses_count', 'retweeted_status.user.lang', 'retweeted_status.user.contributors_enabled', 'retweeted_status.user.is_translator', 'retweeted_status.user.is_translation_enabled', 'retweeted_status.user.profile_background_color', 'retweeted_status.user.profile_background_image_url', 'retweeted_status.user.profile_background_image_url_https', 'retweeted_status.user.profile_background_tile', 'retweeted_status.user.profile_image_url', 'retweeted_status.user.profile_image_url_https', 'retweeted_status.user.profile_banner_url', 'retweeted_status.user.profile_link_color', 'retweeted_status.user.profile_sidebar_border_color', 'retweeted_status.user.profile_sidebar_fill_color', 'retweeted_status.user.profile_text_color', 'retweeted_status.user.profile_use_background_image', 'retweeted_status.user.has_extended_profile', 'retweeted_status.user.default_profile', 'retweeted_status.user.default_profile_image', 'retweeted_status.user.following', 'retweeted_status.user.follow_request_sent', 'retweeted_status.user.notifications', 'retweeted_status.user.translator_type', 'retweeted_status.user.withheld_in_countries', 'retweeted_status.geo', 'retweeted_status.coordinates', 'retweeted_status.place', 'retweeted_status.contributors', 'retweeted_status.is_quote_status', 'retweeted_status.retweet_count', 'retweeted_status.favorite_count', 'retweeted_status.favorited', 'retweeted_status.retweeted', 'retweeted_status.possibly_sensitive', 'retweeted_status.lang', 'retweeted_status.user.entities.url.urls', 'quoted_status_id', 'quoted_status_id_str', 'quoted_status.created_at', 'quoted_status.id', 'quoted_status.id_str', 'quoted_status.full_text', 'quoted_status.truncated', 'quoted_status.display_text_range', 'quoted_status.entities.hashtags', 'quoted_status.entities.symbols', 'quoted_status.entities.user_mentions', 'quoted_status.entities.urls', 'quoted_status.metadata.iso_language_code', 'quoted_status.metadata.result_type', 'quoted_status.source', 'quoted_status.in_reply_to_status_id', 'quoted_status.in_reply_to_status_id_str', 'quoted_status.in_reply_to_user_id', 'quoted_status.in_reply_to_user_id_str', 'quoted_status.in_reply_to_screen_name', 'quoted_status.user.id', 'quoted_status.user.id_str', 'quoted_status.user.name', 'quoted_status.user.screen_name', 'quoted_status.user.location', 'quoted_status.user.description', 'quoted_status.user.url', 'quoted_status.user.entities.url.urls', 'quoted_status.user.entities.description.urls', 'quoted_status.user.protected', 'quoted_status.user.followers_count', 'quoted_status.user.friends_count', 'quoted_status.user.listed_count', 'quoted_status.user.created_at', 'quoted_status.user.favourites_count', 'quoted_status.user.utc_offset', 'quoted_status.user.time_zone', 'quoted_status.user.geo_enabled', 'quoted_status.user.verified', 'quoted_status.user.statuses_count', 'quoted_status.user.lang', 'quoted_status.user.contributors_enabled', 'quoted_status.user.is_translator', 'quoted_status.user.is_translation_enabled', 'quoted_status.user.profile_background_color', 'quoted_status.user.profile_background_image_url', 'quoted_status.user.profile_background_image_url_https', 'quoted_status.user.profile_background_tile', 'quoted_status.user.profile_image_url', 'quoted_status.user.profile_image_url_https', 'quoted_status.user.profile_banner_url', 'quoted_status.user.profile_link_color', 'quoted_status.user.profile_sidebar_border_color', 'quoted_status.user.profile_sidebar_fill_color', 'quoted_status.user.profile_text_color', 'quoted_status.user.profile_use_background_image', 'quoted_status.user.has_extended_profile', 'quoted_status.user.default_profile', 'quoted_status.user.default_profile_image', 'quoted_status.user.following', 'quoted_status.user.follow_request_sent', 'quoted_status.user.notifications', 'quoted_status.user.translator_type', 'quoted_status.user.withheld_in_countries', 'quoted_status.geo', 'quoted_status.coordinates', 'quoted_status.place', 'quoted_status.contributors', 'quoted_status.is_quote_status', 'quoted_status.retweet_count', 'quoted_status.favorite_count', 'quoted_status.favorited', 'quoted_status.retweeted', 'quoted_status.lang', 'place.id', 'place.url', 'place.place_type', 'place.name', 'place.full_name', 'place.country_code', 'place.country', 'place.contained_within', 'place.bounding_box.type', 'place.bounding_box.coordinates', 'geo.type', 'geo.coordinates', 'coordinates.type', 'coordinates.coordinates', 'quoted_status.quoted_status_id', 'quoted_status.quoted_status_id_str', 'quoted_status.possibly_sensitive']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all column names\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# Print the list of column names\n",
    "print(all_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base column names you want to keep\n",
    "# copy and paste from the output above to make sure you get the column names correct\n",
    "base_columns_to_keep = ['created_at', 'id', 'full_text', 'in_reply_to_screen_name']\n",
    "\n",
    "# Add the metadata and user columns to the list since those are somewhat interesting to us\n",
    "columns_to_keep = base_columns_to_keep + [col for col in all_columns if col.startswith('metadata.') or col.startswith('user.')]\n",
    "\n",
    "# Keep only the desired columns in the DataFrame\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# print out the results to a csv file to check them\n",
    "df.to_csv('limitedcolumns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at limitedcolumns.csv, it looks like we can safely remove some columns\n",
    "# Let's remove the columns that have no data in them\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# print out the results to a csv file to check them\n",
    "df.to_csv('limitedcolumns_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's remove the columns that have the word \"url\" or \"color\" in them since they don't seem to be useful\n",
    "df = df[df.columns.drop(list(df.filter(regex='url|color')))]\n",
    "\n",
    "# print out the results to a csv file to check them\n",
    "df.to_csv('limitedcolumns_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the withheld_in_countries column because it doesn't have usable values\n",
    "df.drop(columns=['user.withheld_in_countries'], inplace=True)\n",
    "\n",
    "# let's also change all the columns with boolean values to be 1 or 0 instead of True or False\n",
    "# this makes it easier to work with the data later\n",
    "df = df.replace({True: 1, False: 0})\n",
    "\n",
    "# we aren't interested in tweets from sams's club and there is another account called SamsClub_Sam that we want to get rid of\n",
    "# since we have a pretty good size dataset for our purposes let's just get rid of any row that has a username with the words \"SamsClub\" in it regardless of case\n",
    "# Remove rows where 'user.screen_name' or 'user.name' contains 'samsclub'\n",
    "df = df[~(df['user.screen_name'].str.contains('samsclub', case=False) | df['user.name'].str.contains('samsclub', case=False))]\n",
    "\n",
    "\n",
    "# print out the results to a csv file to check them\n",
    "df.to_csv('limitedcolumns_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have trimmed the data down to 27 columns now and the data looks much more manageable\n",
    "# a couple of more cleanup items and we will be ready to start analyzing the data\n",
    "# let's convert all the column names to lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# finally, let's replace any \".\" in the column names with \"_\" to be consistent\n",
    "df.columns = df.columns.str.replace('.', '_')\n",
    "\n",
    "# print out the results to a csv file to check them\n",
    "df.to_csv('limitedcolumns_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment and Emotion Analysis\n",
    "Now on to the first set of analysis: \n",
    "For each Tweet, we need to find out the main sentiment (Positive, Neutral, Negative) and the main emotion (Joy, Surprise, Neutral, Sadness, Mistrust, and Disgust)\n",
    "We will want to return, both, the predicted sentiment and emotion, as well as the score (ranging for -1 (negative) to 1 (positive) for Sentiment and 0 (disgust) to 1 (joy) for emotion).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to take in our dataframe and return a new dataframe \n",
    "# with the columns for sentiment and emotion added to each tweet\n",
    "def sentiment_emotion_analysis(df):\n",
    "    # Initialize empty lists to hold sentiment and emotion data\n",
    "    sentiments = []\n",
    "    emotions = []\n",
    "\n",
    "    # Loop through every tweet in the dataframe's 'full_text' column using Few-Shot Prompting\n",
    "    for tweet in df['full_text']:\n",
    "        # Define the prompt to be used for Few-Shot Prompting\n",
    "        # Here is more information on Few-Shot Prompting: https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api \n",
    "        prompt=f\"\"\"\n",
    "        Analyze the sentiment of the following tweet and provide a sentiment rating from -1 (completely negative) to 1 (completely positive) and all values in between. If the sentiment is neutral, provide a score of 0. Also, provide an emotion associated with the tweet, choosing from Joy, Surprise, Neutral, Sadness, Mistrust, and Disgust respectively. Rate the emotions from complete joy(1) down to complete disgust(0), assigning values to the emotions as follows: Joy = 1, Surprise = 0.80, Neutral = 0.60, Sadness = 0.40, Mistrust = 0.20, and Disgust = 0. \n",
    "\n",
    "        Here are some examples:\n",
    "\n",
    "        Tweet: Who is making these decisions @SamsClub Do you hate your employees?? Are you kidding?! The @weatherchannel is here. Put it on tv, pay attention to what they are saying, and tell your employees to stay home! #HurricaneIdalia\n",
    "        Sentiment: -0.8\n",
    "        Emotion: Disgust\n",
    "\n",
    "        Tweet: Sams club has frozen grill cheese sandwiches ü§îü§îsheer genius üòçüòç\n",
    "        Sentiment: 0.8\n",
    "        Emotion: Joy\n",
    "\n",
    "        Tweet: signed up for a sams club membership. this feel wayyy too grown for me\n",
    "        Sentiment: -0.2\n",
    "        Emotion: Sadness\n",
    "\n",
    "        Tweet: I really need to go sams club\n",
    "        sentiment: 0.0\n",
    "        Emotion: Neutral\n",
    "\n",
    "        Tweet: moms coming home@with sams club pizza!!\n",
    "        Sentiment: 0.9\n",
    "        Emotion: Surprise\n",
    "\n",
    "        Provide the sentiment and emotion ratings in the following JSON format:\n",
    "        {{\"Sentiment\": \"<sentiment>\", \"Sentiment_Value\": <sentiment_value>, \"Emotion\": \"<emotion>\", \"Emotion_Value\": <emotion_value>}}\n",
    "\n",
    "        What is the sentiment and emotion of this tweet:\n",
    "        \"{tweet}\"\n",
    "\n",
    "        Answer: {{\"Sentiment\": \"<sentiment>\", \"Sentiment_Value\": <sentiment_value>, \"Emotion\": \"<emotion>\", \"Emotion_Value\": <emotion_value>}}\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Create an API call to OpenAI using the specified model, prompt, and parameters\n",
    "        # We can play with the parameters to see if we can get better results\n",
    "        response = openai.Completion.create(\n",
    "            # This is the model we want to use to generate the content - in this case we are using a ChatGPT 3.5 model because it is cheaper to use than the GPT-4 model\n",
    "            # Useful for prototyping and testing then using the GPT-4 model for later iterations\n",
    "            # At the time of this writing, for input the GPT-3.5 model costs $0.0015 per 1000 tokens and the GPT-4 model costs $0.03 per 1000 tokens; for output the \n",
    "            # GPT-3.5 model costs $0.002 per 1000 tokens and the GPT-4 model costs $0.06 per 1000 tokens\n",
    "            # Quite a cost differential between the two models so start cheap to prototype and test and then use the GPT-4 model for later on\n",
    "            model=\"text-davinci-003\",  \n",
    "\n",
    "            # This is the prompt we created above\n",
    "            prompt=prompt,  \n",
    "\n",
    "            # This is the temperature parameter - it controls the randomness of the output or, put another way, how \"creative\" the AI is\n",
    "            # The number can be between 0 and 1 - the closer to 0 the less \"creative\" the AI is and as the number approaches 1 the more \"creative\" the AI is\n",
    "            # Since we are doing an objective analysis we want the AI to be less creative so we set the temperature to 0.2\n",
    "            # If we were creating children's stories, for example, we would want the AI to be more creative so we set the temperature to 0.7\n",
    "            temperature=0.2,  \n",
    "\n",
    "            # This is the max_tokens parameter - The maximum number of tokens to generate in the completion. \n",
    "            # The token count of your prompt plus max_tokens cannot exceed the model‚Äôs context length. \n",
    "            # Most models have a context length of 2048 tokens (except for the newest models, which support 4096).\n",
    "            # To quote OpenAI: \"You can think of tokens as pieces of words, where 1,000 tokens is about 750 words.\"\n",
    "            # Our output is fairly small per try so we set the max_tokens to 200 to give the AI enough room to generate the content\n",
    "            # But small enough to keep the cost down or, at least, error out so can know the limit we set is too small\n",
    "            max_tokens=2000,  \n",
    "\n",
    "            # This is the top_p parameter - it controls the range of words chosen for the output\n",
    "            # The values are from 0 to 1 - the closer to 0 the less diverse the output and the closer to 1 the more diverse the output\n",
    "            # For example, if we set top_p to 0.5 then the AI will only use the top 50% of the most likely words\n",
    "            # But, if we set top_p to 1.0 then the AI will use all of the words\n",
    "            # We are doing an objective analysis so we want the AI to be less diverse so we set the top_p to 0.5\n",
    "            # If we were creating children's stories we would want the AI to be more diverse so we set the top_p to 1.0\n",
    "            top_p=0.5,  \n",
    "\n",
    "            # This is the frequency_penalty parameter - this parameter is used to discourage the model from repeating the same words or phrases too frequently within the generated text.\n",
    "            # A higher frequency_penalty value will result in the model being more conservative in its use of repeated words. \n",
    "            # The values are from -2.0 to 2.0 - the closer to -2.0 the more likely the AI will repeat the same words and the closer to 2.0 the less likely the AI will repeat the same words\n",
    "            # Typical setting for this parameter is 0.0 or to 1 for eliminating repetition in output.\n",
    "            frequency_penalty=0.0,  \n",
    "\n",
    "            # This is the presence_penalty parameter - this parameter is used to encourage the model to include a diverse range of words in the generated text. \n",
    "            # A higher presence_penalty value will result in the model being more likely to generate words that have not yet been included in the generated text.\n",
    "            # The values are from -2.0 to 2.0 - the closer to -2.0 the more likely the AI will repeat the same words and the closer to 2.0 the more likely to include words not used before\n",
    "            # As with the frequency_penalty parameter, typical setting for this parameter is 0.0 or to 1 for eliminating repetition in output. \n",
    "            presence_penalty=0.0 \n",
    "        )\n",
    "\n",
    "        # The hard part is done - now we just need to process the response from the API\n",
    "        # Get the response text and remove the text \"Answer: \" from the beginning of the response\n",
    "        response_text = response['choices'][0]['text'].strip().replace(\"Answer: \", \"\")\n",
    "\n",
    "        # Process the response text\n",
    "        try:\n",
    "            # Load the response into a JSON object\n",
    "            response_json = json.loads(response_text)\n",
    "\n",
    "            # Extract the sentiment and emotion data from the JSON object\n",
    "            sentiment = response_json[\"Sentiment\"]\n",
    "            sentiment_value = response_json[\"Sentiment_Value\"]\n",
    "            emotion = response_json[\"Emotion\"]\n",
    "            emotion_value = response_json[\"Emotion_Value\"]\n",
    "\n",
    "            # Append the sentiment and emotion data to the corresponding lists we created earlier\n",
    "            sentiments.append((sentiment, sentiment_value))\n",
    "            emotions.append((emotion, emotion_value))\n",
    "\n",
    "        # If there's an error processing the response text, append \"Error\" and 0 to the data\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing tweet: {tweet}\")\n",
    "            print(f\"Response from API: {response_text}\")\n",
    "            sentiments.append((\"Error\", 0))\n",
    "            emotions.append((\"Error\", 0))\n",
    "\n",
    "    # Add the sentiment and emotion data to the dataframe as new columns\n",
    "    df['sentiment'], df['sentiment_value'] = zip(*sentiments)\n",
    "    df['emotion'], df['emotion_value'] = zip(*emotions)\n",
    "\n",
    "    # Return the updated dataframe\n",
    "    return df\n",
    "\n",
    "# Call the function and update the dataframe\n",
    "df = sentiment_emotion_analysis(df)\n",
    "\n",
    "# Save the updated dataframe as a CSV file to check the results\n",
    "df.to_csv('SentimentEmotion.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction\n",
    "To enhance our data for analysis, the business has asked for each Tweet to be assigned a topic. This will aid in analysis later on and make it easier to work with segments of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named get_topic that takes a tweet text as an input and determines the topic\n",
    "def get_topic(text):\n",
    "    # work up our prompt to be used for Few-Shot Prompting\n",
    "    prompt = f\"\"\"\n",
    "    Given the following tweet, determine the topic or subject in one or two words. The topic should be broad enough to cover a large number of tweets, but specific enough to be useful. Don't include any punctuation at the end of the topic.\n",
    "\n",
    "    Here are some examples:\n",
    "\n",
    "    Tweet: Sams club scan & go for the gas station and the store is the best shit ever.\n",
    "    The topic of this tweet is: Scan & Go\n",
    "\n",
    "    Tweet: I didn‚Äôt know I needed to invest in bath towels until I got some new ones from sams club ü§©\n",
    "    The topic of this tweet is: Bath Towels\n",
    "\n",
    "    Tweet: Disneyland animatronic dragons: 0, Sam‚Äôs Club animatronic dragons: 1\n",
    "    Topic: Inflatables / Airblown\n",
    "\n",
    "    Tweet: Sam‚Äôs Club‚Äôs $1.38 hot dog combo powers it past Costco in sales war \n",
    "    Topic: Cafe Food\n",
    "\n",
    "    Tweet: @RateYourCharge but your charge update here at Sam's club in Salt Lake City. We tried to charge on charger 7 plugged in a failed to initialize. Now I plugged in to charge your five and sells welcome Joy which is not me which is kind of weird but it seems to be charging now.\n",
    "    Topic: EV Charging\n",
    "\n",
    "    Tweet: \"{text}\"\n",
    "\n",
    "    The topic of this tweet is:\"\"\"\n",
    "    \n",
    "    # Create an API call to OpenAI using the specified model, prompt, and parameters\n",
    "    # We can play with the parameters to see if we can get better results\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",  \n",
    "        prompt=prompt,  \n",
    "        temperature=0.6, \n",
    "        max_tokens=200,  \n",
    "        top_p=1.0,  \n",
    "        frequency_penalty=0, \n",
    "        presence_penalty=0  \n",
    "        \n",
    "    )\n",
    "     # The model's response is processed to extract the text, remove unnecessary strings and strip leading/trailing white spaces.\n",
    "    response_text = response['choices'][0]['text'].strip().replace(\"Answer: \", \"\")\n",
    "    \n",
    "    # The processed response text, which is the predicted topic, is returned from the function.\n",
    "    return response_text\n",
    "\n",
    "# The get_topic function is applied to the 'full_text' column and the results are stored in a new column 'topic'.\n",
    "df['topic'] = df['full_text'].apply(get_topic)\n",
    "\n",
    "# The training dataframe with the new 'topic' column is saved to a CSV file \n",
    "df.to_csv('TopicExtraction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification / Categorization\n",
    "\n",
    "To further enhance the data the analytics department wants us to categorize each Tweet based on it's content into one of the following categories: Content Quality, Customer Support, Spam, Membership issues, Marketing and Other. For each Tweet, we should calculate the category and a score associated tho the category that shows the intensity of the category chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to take in our tweet and categorize it\n",
    "def categorize_tweet(text):\n",
    "    # A Few-Shot prompt is defined which includes the tweet and instructions for the model to categorize the tweet.\n",
    "    prompt = f\"\"\"\n",
    "    Given the following tweet, please categorize it into one of the following categories with a score from 0 (not relevant) to 1 (highly relevant) and all values in between based on how relevant the tweet is in relation to the category. \n",
    "\n",
    "    Examples:\n",
    "    \n",
    "    Tweet: Finna go to sams club and get a box of nature valley bars\n",
    "    Category: Other\n",
    "    Score: 0.5\n",
    "\n",
    "    Tweet: üö® BRAND NEW JUNE @SamsClub 2023 VIDEO üëâüèº\n",
    "    Category: Marketing\n",
    "    Score: 1\n",
    "\n",
    "    Tweet: Sam‚Äôs Club membership expires soon. May join Costco for a year to compare. ü§î\n",
    "    Category: Membership issues\n",
    "    Score: 0.8\n",
    "\n",
    "    Tweet: Person‚Äôs complaint is on spam emails from ADT, along w Sam‚Äôs Club, Chase Bank, Wells Fargo, etc. sent to his inbox. Also mentions a hack to ADT‚Äôs systems.\n",
    "    Category: Spam\n",
    "    Score: 0.9\n",
    "\n",
    "    Tweet: I sent a Email Tuesday to customer support  because I did not see the Add an offer code' visible in the Sam‚Äôs club site at check out I haven‚Äôt received a reply yet\n",
    "    Category: Customer Support\n",
    "    Score: 0.7\n",
    "\n",
    "    Tweet: Sam's Club sent me an email notice of recall for a readily consumed product. The contaminants both likely lethal, including Clostridium botulinum.  BOTULISM! \n",
    "    Category: Content Quality\n",
    "    Score: 1\n",
    "\n",
    "\n",
    "    Please provide the category and score in the following JSON format:\n",
    "    {{ \"Category\": \"<category_name>\", \"Score\": \"<score>\" }}\n",
    "    \n",
    "    The categories are: \n",
    "\n",
    "    - Content Quality\n",
    "    - Customer Support\n",
    "    - Spam\n",
    "    - Membership issues\n",
    "    - Marketing\n",
    "    - Other\n",
    "\n",
    "    Tweet: \"{text}\"\n",
    "\n",
    "    Answer: <json output>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an API call to OpenAI using the specified model, prompt, and parameters\n",
    "    # We can play with the parameters to see if we can get better results\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",  \n",
    "        prompt=prompt,  \n",
    "        temperature=0.2,  \n",
    "        max_tokens=2000,  \n",
    "        top_p=0.5,  \n",
    "        frequency_penalty=0.0,  \n",
    "        presence_penalty=0.0  \n",
    "    )\n",
    "    \n",
    "    # process the response to extract the text and remove unnecessary text\n",
    "    response_text = response['choices'][0]['text'].strip().replace(\"Answer: \", \"\")\n",
    "\n",
    "     # convert the response to a Python dictionary using json.loads\n",
    "    result_dict = json.loads(response_text)\n",
    "    \n",
    "    # The category and score are extracted from the dictionary and returned from the function\n",
    "    category = result_dict[\"Category\"]\n",
    "    score = float(result_dict[\"Score\"])\n",
    "    return category, score\n",
    "\n",
    "# The categorize_tweet function is applied to the 'full_text' column of the dataframe. The returned category and score are stored in two new columns in the dataframe.\n",
    "# By calling zip(*), we unpack the data into two separate series, which are then separately assigned to the new DataFrame columns 'category' and 'category_score'. \n",
    "df['category'], df['category_score'] = zip(*df['full_text'].apply(categorize_tweet))\n",
    "\n",
    "# The updated dataframe, with the new 'category' and 'category_score' columns, is saved to a CSV file\n",
    "df.to_csv('Classification.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result\n",
    "\n",
    "Here we may decide to trim out a few more columns or clean up the output a little more but, for the most part, we are done with this basic overview. I'll make one more change to a column name that is bugging me and call it good for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['created_at', 'tweet_id', 'full_text', 'in_reply_to_screen_name',\n",
      "       'metadata_iso_language_code', 'metadata_result_type', 'user_id',\n",
      "       'user_id_str', 'user_name', 'user_screen_name', 'user_location',\n",
      "       'user_description', 'user_protected', 'user_followers_count',\n",
      "       'user_friends_count', 'user_listed_count', 'user_created_at',\n",
      "       'user_favourites_count', 'user_geo_enabled', 'user_verified',\n",
      "       'user_statuses_count', 'user_contributors_enabled',\n",
      "       'user_is_translator', 'user_is_translation_enabled',\n",
      "       'user_profile_background_tile', 'user_profile_use_background_image',\n",
      "       'user_has_extended_profile', 'user_default_profile',\n",
      "       'user_default_profile_image', 'user_following',\n",
      "       'user_follow_request_sent', 'user_notifications',\n",
      "       'user_translator_type', 'sentiment', 'sentiment_value', 'emotion',\n",
      "       'emotion_value', 'topic', 'category', 'category_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# replace the column name 'id' with 'tweet_id'\n",
    "df = df.rename(columns={'id': 'tweet_id'})\n",
    "\n",
    "\n",
    "# Now print the columns to check the change and see if any other column names are bugging us\n",
    "print(df.columns)\n",
    "\n",
    "# save the dataframe to a CSV file\n",
    "df.to_csv('FinalResult.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
