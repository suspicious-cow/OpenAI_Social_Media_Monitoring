{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Media Monitoring and Analysis\n",
    "\n",
    "Welcome! This notebook is geared to give you starter examples of how to use ChatGPT for social media monitoring and analysis. Specifically we will look at three use cases:\n",
    "- Sentiment and Emotion Analysis\n",
    "- Topic Extraction\n",
    "- Classification / Categorization\n",
    "\n",
    "For our purposes we will assume we work at Sam's Club (a well-known retail warehouse chain owned by Wal-Mart) and have been asked to look at a series of tweets for our analysis. I'll walk you through all the basic steps needed to do the analysis. \n",
    "\n",
    "NOTE: To make it easier to focus on the process of analysis instead of acquiring the data, I've included a JSON file that has curated output from Twitter (now called, X) in it. If you want to sign up for a developer account so you can get live data, go here: https://developer.twitter.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment and Emotion Analysis\n",
    "\n",
    "The first order of business is to take the data and put it into a format that is easier to analyze. Take a moment to look at the Tweets.json file and see how we get information from Twitter. Notice that the file is a list of dictionaries where each dictionary represents a tweet. We need to manipulate it into something usable. Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our packages \n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# set our key for openai to be used later\n",
    "openai_key = os.getenv('OPENAI_KEY')\n",
    "openai.api_key = openai_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Data\n",
    "\n",
    "Now that we have our packages ready to go and our OpenAI API key set we need to read the data info a pandas dataframe to ease our cleaning and analysis efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by inspecting the json file and determining the structure of the data. Remember that the file is a list of dictionaries where each dictionary represents a tweet. \n",
    "# Some are nested dictionaries and some are not. We need to flatten the data so that we can convert it into a DataFrame.\n",
    "# The json_normalize function from pandas is used to convert JSON data into a flat table (DataFrame).\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Open the file named \"Tweets.json\" for reading. The \"with\" statement ensures that the file is properly closed after it is no longer needed.\n",
    "with open(\"Tweets.json\") as file:\n",
    "    # Use the json.load function to load the JSON data from the file into a Python object (usually a list or a dictionary).\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert the JSON data into a DataFrame. json_normalize flattens the data, meaning it can create a DataFrame from nested JSON data.\n",
    "df = json_normalize(data)\n",
    "\n",
    "# Print the first 5 rows of the DataFrame to see what the data looks like.\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame to a CSV file named \"initaldataframe.csv\" so you can see the progress. The argument index=False means that the DataFrame's index will not be saved in the CSV file.\n",
    "# While this step isn't necessary, it's a good idea to save your work as you go along and to check that the data looks correct.\n",
    "# If you are using VS Code, I highly recommend installing the Excel Viewer extension or Rainbow CSV extension so you can view the CSV file more easily.\n",
    "df.to_csv('initaldataframe.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data Up\n",
    "\n",
    "We will do some very simple cleanup to make dealing with our data easier. First, let's take out columns that we obviously won't need for our goals to make the data even more easy to analyze. Take a look at the initaldataframe.csv and determine what columns you think should be kept for our analysis. At this early stage it's usually a good idea to keep a column if in doubt. We can always trim it out later. Since we are keeping it simple we can hack and slash a bit more than usual. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all column names\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# Print the list of column names\n",
    "print(all_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base column names you want to keep\n",
    "# copy and paste from the output above to make sure you get the column names correct\n",
    "base_columns_to_keep = ['created_at', 'id', 'full_text', 'in_reply_to_screen_name']\n",
    "\n",
    "# Add the metadata and user columns to the list since those are somewhat interesting to us\n",
    "columns_to_keep = base_columns_to_keep + [col for col in all_columns if col.startswith('metadata.') or col.startswith('user.')]\n",
    "\n",
    "# Keep only the desired columns in the DataFrame\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# print out the results to a csv file to check them\n",
    "df.to_csv('limitedcolumns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at limitedcolumns.csv, it looks like we can safely remove some columns\n",
    "# Let's remove the columns that have no data in them\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# print out the results to a csv file to check them\n",
    "df.to_csv('limitedcolumns_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's remove the columns that have the word \"url\" or \"color\" in them since they don't seem to be useful\n",
    "df = df[df.columns.drop(list(df.filter(regex='url|color')))]\n",
    "\n",
    "# print out the results to a csv file to check them\n",
    "df.to_csv('limitedcolumns_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the withheld_in_countries column because it doesn't have usable values\n",
    "df.drop(columns=['user.withheld_in_countries'], inplace=True)\n",
    "\n",
    "# let's also change all the columns with boolean values to be 1 or 0 instead of True or False\n",
    "# this makes it easier to work with the data later\n",
    "df = df.replace({True: 1, False: 0})\n",
    "\n",
    "# we aren't interested in tweets from sams's club and there is another account called SamsClub_Sam that we want to get rid of\n",
    "# since we have a pretty good size dataset for our purposes let's just get rid of any row that has a username with the words \"SamsClub\" in it regardless of case\n",
    "# Remove rows where 'user.screen_name' or 'user.name' contains 'samsclub'\n",
    "df = df[~(df['user.screen_name'].str.contains('samsclub', case=False) | df['user.name'].str.contains('samsclub', case=False))]\n",
    "\n",
    "\n",
    "# print out the results to a csv file to check them\n",
    "df.to_csv('limitedcolumns_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have trimmed the data down to 27 columns now and the data looks much more manageable\n",
    "# a couple of more cleanup items and we will be ready to start analyzing the data\n",
    "# let's convert all the column names to lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# finally, let's replace any \".\" in the column names with \"_\" to be consistent\n",
    "df.columns = df.columns.str.replace('.', '_')\n",
    "\n",
    "# print out the results to a csv file to check them\n",
    "df.to_csv('limitedcolumns_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment and Emotion Analysis\n",
    "Now on to the first set of analysis: \n",
    "For each Tweet, we need to find out the main sentiment (Positive, Neutral, Negative) and the main emotion (Joy, Surprise, Neutral, Sadness, Mistrust, and Disgust)\n",
    "We will want to return, both, the predicted sentiment and emotion, as well as the score (ranging for -1 (negative) to 1 (positive) for Sentiment and 0 (disgust) to 1 (joy) for emotion).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to take in our dataframe and return a new dataframe \n",
    "# with the columns for sentiment and emotion added to each tweet\n",
    "def sentiment_emotion_analysis(df):\n",
    "    # Initialize empty lists to hold sentiment and emotion data\n",
    "    sentiments = []\n",
    "    emotions = []\n",
    "\n",
    "    # Loop through every tweet in the dataframe's 'full_text' column using Few-Shot Prompting\n",
    "    for tweet in df['full_text']:\n",
    "        # Define the prompt to be used for Few-Shot Prompting\n",
    "        # Here is more information on Few-Shot Prompting: https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api \n",
    "        prompt=f\"\"\"\n",
    "        Analyze the sentiment of the following tweet and provide a sentiment rating from -1 (completely negative) to 1 (completely positive) and all values in between. If the sentiment is neutral, provide a score of 0. Also, provide an emotion associated with the tweet, choosing from Joy, Surprise, Neutral, Sadness, Mistrust, and Disgust respectively. Rate the emotions from complete joy(1) down to complete disgust(0), assigning values to the emotions as follows: Joy = 1, Surprise = 0.80, Neutral = 0.60, Sadness = 0.40, Mistrust = 0.20, and Disgust = 0. \n",
    "\n",
    "        Here are some examples:\n",
    "\n",
    "        Tweet: Who is making these decisions @SamsClub Do you hate your employees?? Are you kidding?! The @weatherchannel is here. Put it on tv, pay attention to what they are saying, and tell your employees to stay home! #HurricaneIdalia\n",
    "        Sentiment: -0.8\n",
    "        Emotion: Disgust\n",
    "\n",
    "        Tweet: Sams club has frozen grill cheese sandwiches 🤔🤔sheer genius 😍😍\n",
    "        Sentiment: 0.8\n",
    "        Emotion: Joy\n",
    "\n",
    "        Tweet: signed up for a sams club membership. this feel wayyy too grown for me\n",
    "        Sentiment: -0.2\n",
    "        Emotion: Sadness\n",
    "\n",
    "        Tweet: I really need to go sams club\n",
    "        sentiment: 0.0\n",
    "        Emotion: Neutral\n",
    "\n",
    "        Tweet: moms coming home@with sams club pizza!!\n",
    "        Sentiment: 0.9\n",
    "        Emotion: Surprise\n",
    "\n",
    "        Provide the sentiment and emotion ratings in the following JSON format:\n",
    "        {{\"Sentiment\": \"<sentiment>\", \"Sentiment_Value\": <sentiment_value>, \"Emotion\": \"<emotion>\", \"Emotion_Value\": <emotion_value>}}\n",
    "\n",
    "        What is the sentiment and emotion of this tweet:\n",
    "        \"{tweet}\"\n",
    "\n",
    "        Answer: {{\"Sentiment\": \"<sentiment>\", \"Sentiment_Value\": <sentiment_value>, \"Emotion\": \"<emotion>\", \"Emotion_Value\": <emotion_value>}}\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Create an API call to OpenAI using the specified model, prompt, and parameters\n",
    "        # We can play with the parameters to see if we can get better results\n",
    "        response = openai.Completion.create(\n",
    "            # This is the model we want to use to generate the content - in this case we are using a ChatGPT 3.5 model because it is cheaper to use than the GPT-4 model\n",
    "            # Useful for prototyping and testing then using the GPT-4 model for later iterations\n",
    "            # At the time of this writing, for input the GPT-3.5 model costs $0.0015 per 1000 tokens and the GPT-4 model costs $0.03 per 1000 tokens; for output the \n",
    "            # GPT-3.5 model costs $0.002 per 1000 tokens and the GPT-4 model costs $0.06 per 1000 tokens\n",
    "            # Quite a cost differential between the two models so start cheap to prototype and test and then use the GPT-4 model for later on\n",
    "            model=\"text-davinci-003\",  \n",
    "\n",
    "            # This is the prompt we created above\n",
    "            prompt=prompt,  \n",
    "\n",
    "            # This is the temperature parameter - it controls the randomness of the output or, put another way, how \"creative\" the AI is\n",
    "            # The number can be between 0 and 1 - the closer to 0 the less \"creative\" the AI is and as the number approaches 1 the more \"creative\" the AI is\n",
    "            # Since we are doing an objective analysis we want the AI to be less creative so we set the temperature to 0.2\n",
    "            # If we were creating children's stories, for example, we would want the AI to be more creative so we set the temperature to 0.7\n",
    "            temperature=0.2,  \n",
    "\n",
    "            # This is the max_tokens parameter - The maximum number of tokens to generate in the completion. \n",
    "            # The token count of your prompt plus max_tokens cannot exceed the model’s context length. \n",
    "            # Most models have a context length of 2048 tokens (except for the newest models, which support 4096).\n",
    "            # To quote OpenAI: \"You can think of tokens as pieces of words, where 1,000 tokens is about 750 words.\"\n",
    "            # Our output is fairly small per try so we set the max_tokens to 200 to give the AI enough room to generate the content\n",
    "            # But small enough to keep the cost down or, at least, error out so can know the limit we set is too small\n",
    "            max_tokens=2000,  \n",
    "\n",
    "            # This is the top_p parameter - it controls the range of words chosen for the output\n",
    "            # The values are from 0 to 1 - the closer to 0 the less diverse the output and the closer to 1 the more diverse the output\n",
    "            # For example, if we set top_p to 0.5 then the AI will only use the top 50% of the most likely words\n",
    "            # But, if we set top_p to 1.0 then the AI will use all of the words\n",
    "            # We are doing an objective analysis so we want the AI to be less diverse so we set the top_p to 0.5\n",
    "            # If we were creating children's stories we would want the AI to be more diverse so we set the top_p to 1.0\n",
    "            top_p=0.5,  \n",
    "\n",
    "            # This is the frequency_penalty parameter - this parameter is used to discourage the model from repeating the same words or phrases too frequently within the generated text.\n",
    "            # A higher frequency_penalty value will result in the model being more conservative in its use of repeated words. \n",
    "            # The values are from -2.0 to 2.0 - the closer to -2.0 the more likely the AI will repeat the same words and the closer to 2.0 the less likely the AI will repeat the same words\n",
    "            # Typical setting for this parameter is 0.0 or to 1 for eliminating repetition in output.\n",
    "            frequency_penalty=0.0,  \n",
    "\n",
    "            # This is the presence_penalty parameter - this parameter is used to encourage the model to include a diverse range of words in the generated text. \n",
    "            # A higher presence_penalty value will result in the model being more likely to generate words that have not yet been included in the generated text.\n",
    "            # The values are from -2.0 to 2.0 - the closer to -2.0 the more likely the AI will repeat the same words and the closer to 2.0 the more likely to include words not used before\n",
    "            # As with the frequency_penalty parameter, typical setting for this parameter is 0.0 or to 1 for eliminating repetition in output. \n",
    "            presence_penalty=0.0 \n",
    "        )\n",
    "\n",
    "        # The hard part is done - now we just need to process the response from the API\n",
    "        # Get the response text and remove the text \"Answer: \" from the beginning of the response\n",
    "        response_text = response['choices'][0]['text'].strip().replace(\"Answer: \", \"\")\n",
    "\n",
    "        # Process the response text\n",
    "        try:\n",
    "            # Load the response into a JSON object\n",
    "            response_json = json.loads(response_text)\n",
    "\n",
    "            # Extract the sentiment and emotion data from the JSON object\n",
    "            sentiment = response_json[\"Sentiment\"]\n",
    "            sentiment_value = response_json[\"Sentiment_Value\"]\n",
    "            emotion = response_json[\"Emotion\"]\n",
    "            emotion_value = response_json[\"Emotion_Value\"]\n",
    "\n",
    "            # Append the sentiment and emotion data to the corresponding lists we created earlier\n",
    "            sentiments.append((sentiment, sentiment_value))\n",
    "            emotions.append((emotion, emotion_value))\n",
    "\n",
    "        # If there's an error processing the response text, append \"Error\" and 0 to the data\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing tweet: {tweet}\")\n",
    "            print(f\"Response from API: {response_text}\")\n",
    "            sentiments.append((\"Error\", 0))\n",
    "            emotions.append((\"Error\", 0))\n",
    "\n",
    "    # Add the sentiment and emotion data to the dataframe as new columns\n",
    "    df['sentiment'], df['sentiment_value'] = zip(*sentiments)\n",
    "    df['emotion'], df['emotion_value'] = zip(*emotions)\n",
    "\n",
    "    # Return the updated dataframe\n",
    "    return df\n",
    "\n",
    "# Call the function and update the dataframe\n",
    "df = sentiment_emotion_analysis(df)\n",
    "\n",
    "# Save the updated dataframe as a CSV file to check the results\n",
    "df.to_csv('SentimentEmotion.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction\n",
    "\n",
    "Now the business has asked us to identify the broader topic discussed for each tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inser your code here\n",
    "\n",
    "def categorize_tweet(text):\n",
    "    # A prompt is defined which includes the tweet text and instructions for the model to categorize the tweet.\n",
    "    # The prompt also includes a few examples to guide the model.\n",
    "    # Finally, the model is asked to return the category and score in JSON format.\n",
    "    prompt = f\"\"\"\n",
    "    Given the following tweet, please categorize it into one of the following categories with a score from 0 (not relevant) to 1 (highly relevant) and all values in between based on how relevant the tweet is in relation to the category. \n",
    "\n",
    "    Examples:\n",
    "    \n",
    "    Tweet: Finna go to sams club and get a box of nature valley bars and open em in this nigga bed\n",
    "    Category: Other\n",
    "    Score: 0.5\n",
    "\n",
    "    Tweet: 🚨 BRAND NEW JUNE @SamsClub 2023 VIDEO 👉🏼\n",
    "    Category: Marketing\n",
    "    Score: 1\n",
    "\n",
    "    Tweet: Northwest Ohio Sam’s Club 4/25/2020.  I want $1.00 a gallon gasoline again!\n",
    "\n",
    "\n",
    "    Please provide the category and score in the following JSON format:\n",
    "    {{ \"Category\": \"<category_name>\", \"Score\": \"<score>\" }}\n",
    "    \n",
    "    The categories are: \n",
    "\n",
    "    - Content Quality\n",
    "    - Customer Support\n",
    "    - Spam\n",
    "    - Membership issues\n",
    "    - Marketing\n",
    "    - Other\n",
    "\n",
    "    Tweet: \"{text}\"\n",
    "\n",
    "    Answer: <json output>\n",
    "    \"\"\"\n",
    "\n",
    "    # The GPT-3 model is invoked with the defined prompt and specific parameters.\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",  \n",
    "        prompt=prompt,  \n",
    "        temperature=0.3,  \n",
    "        max_tokens=100,  \n",
    "        top_p=1.0,  \n",
    "        frequency_penalty=0.0,  \n",
    "        presence_penalty=0.0  \n",
    "    )\n",
    "    \n",
    "    # The model's response is processed to extract the text and remove unnecessary strings.\n",
    "    response_text = response['choices'][0]['text'].strip().replace(\"Answer: \", \"\")\n",
    "\n",
    "     # The remaining response text is a JSON string, which is converted to a Python dictionary using json.loads.\n",
    "    result_dict = json.loads(response_text)\n",
    "    \n",
    "    # The category and score are extracted from the dictionary and returned from the function.\n",
    "    category = result_dict[\"Category\"]\n",
    "    score = float(result_dict[\"Score\"])\n",
    "    return category, score\n",
    "\n",
    "# The categorize_tweet function is applied to the 'full_text' column of the dataframe. The returned category and score are stored in two new columns in the dataframe.\n",
    "# By calling zip(*) on this series, it unpacks these tuples into two separate series, which are then separately assigned to the new DataFrame columns 'category' and 'category_score'. \n",
    "df['category'], df['category_score'] = zip(*df['full_text'].apply(categorize_tweet))\n",
    "\n",
    "# The updated dataframe, with the new 'category' and 'category_score' columns, is saved to a CSV file named 'categorized_tweets.csv'. The index is not included in the file.\n",
    "df.to_csv('TopicExtraction.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
