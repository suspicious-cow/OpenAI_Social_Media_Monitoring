{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Media Monitoring and Analysis\n",
    "\n",
    "Welcome! This notebook is geared to give you starter examples of how to use ChatGPT for social media monitoring and analysis. Specifically we will look at three use cases:\n",
    "- Sentiment and Emotion Analysis\n",
    "- Topic Extraction\n",
    "- Classification / Categorization\n",
    "\n",
    "For our purposes we will assume we work at Sam's Club (a well-known retail warehouse chain owned by Wal-Mart) and have been asked to look at a series of tweets for our analysis. I'll walk you through all the basic steps needed to do the analysis. \n",
    "\n",
    "NOTE: To make it easier to focus on the process of analysis instead of acquiring the data, I've included a JSON file that has curated output from Twitter (now called, X) in it. If you want to sign up for a developer account so you can get live data, go here: https://developer.twitter.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment and Emotion Analysis\n",
    "\n",
    "The first order of business is to take the data and put it into a format that is easier to analyze. Take a moment to look at the Tweets.json file and see how we get information from Twitter. Notice that the file is a list of dictionaries where each dictionary represents a tweet. We need to manipulate it into something usable. Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our packages \n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# set our key for openai to be used later\n",
    "openai_key = os.getenv('OPENAI_KEY')\n",
    "openai.api_key = openai_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Data\n",
    "\n",
    "Now that we have our packages ready to go and our OpenAI API key set we need to read the data info a pandas dataframe to ease our cleaning and analysis efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       created_at                   id               id_str  \\\n",
      "0  Mon May 15 18:21:47 +0000 2023  1658175862045302797  1658175862045302797   \n",
      "1  Mon May 15 17:29:18 +0000 2023  1658162654479888397  1658162654479888397   \n",
      "2  Mon May 15 17:04:13 +0000 2023  1658156342199332864  1658156342199332864   \n",
      "3  Mon May 15 16:54:52 +0000 2023  1658153988703809536  1658153988703809536   \n",
      "4  Mon May 15 16:54:24 +0000 2023  1658153870575521793  1658153870575521793   \n",
      "\n",
      "                                           full_text  truncated  \\\n",
      "0  KitchenAid Mixer Sale on @SamsClub | $90 Off P...      False   \n",
      "1           @KellieShai_ All we need is some milk. üç™      False   \n",
      "2  Happy National Chocolate Chip Day! No denying ...      False   \n",
      "3  This is heartbreaking that less than a year im...      False   \n",
      "4  @VIZIOsupport I bought a vizio tv from @SamsCl...      False   \n",
      "\n",
      "  display_text_range                                             source  \\\n",
      "0           [0, 128]  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
      "1           [13, 40]  <a href=\"https://prod1.sprinklr.com\" rel=\"nofo...   \n",
      "2           [0, 276]  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
      "3           [0, 168]  <a href=\"http://twitter.com/download/android\" ...   \n",
      "4           [0, 279]  <a href=\"http://twitter.com/download/android\" ...   \n",
      "\n",
      "   in_reply_to_status_id in_reply_to_status_id_str  in_reply_to_user_id  ...  \\\n",
      "0                    NaN                      None                  NaN  ...   \n",
      "1           1.658156e+18       1658156342199332864         1.629262e+18  ...   \n",
      "2                    NaN                      None                  NaN  ...   \n",
      "3           1.658154e+18       1658153870575521793         1.620974e+18  ...   \n",
      "4                    NaN                      None         6.915388e+07  ...   \n",
      "\n",
      "  place.contained_within place.bounding_box.type  \\\n",
      "0                    NaN                     NaN   \n",
      "1                    NaN                     NaN   \n",
      "2                    NaN                     NaN   \n",
      "3                    NaN                     NaN   \n",
      "4                    NaN                     NaN   \n",
      "\n",
      "   place.bounding_box.coordinates  geo.type  geo.coordinates coordinates.type  \\\n",
      "0                             NaN       NaN              NaN              NaN   \n",
      "1                             NaN       NaN              NaN              NaN   \n",
      "2                             NaN       NaN              NaN              NaN   \n",
      "3                             NaN       NaN              NaN              NaN   \n",
      "4                             NaN       NaN              NaN              NaN   \n",
      "\n",
      "   coordinates.coordinates  quoted_status.quoted_status_id  \\\n",
      "0                      NaN                             NaN   \n",
      "1                      NaN                             NaN   \n",
      "2                      NaN                             NaN   \n",
      "3                      NaN                             NaN   \n",
      "4                      NaN                             NaN   \n",
      "\n",
      "   quoted_status.quoted_status_id_str  quoted_status.possibly_sensitive  \n",
      "0                                 NaN                               NaN  \n",
      "1                                 NaN                               NaN  \n",
      "2                                 NaN                               NaN  \n",
      "3                                 NaN                               NaN  \n",
      "4                                 NaN                               NaN  \n",
      "\n",
      "[5 rows x 241 columns]\n"
     ]
    }
   ],
   "source": [
    "# Start by inspecting the json file and determining the structure of the data. Remember that the file is a list of dictionaries where each dictionary represents a tweet. \n",
    "# Some are nested dictionaries and some are not. We need to flatten the data so that we can convert it into a DataFrame.\n",
    "# The json_normalize function from pandas is used to convert JSON data into a flat table (DataFrame).\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Open the file named \"Tweets.json\" for reading. The \"with\" statement ensures that the file is properly closed after it is no longer needed.\n",
    "with open(\"Tweets.json\") as file:\n",
    "    # Use the json.load function to load the JSON data from the file into a Python object (usually a list or a dictionary).\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert the JSON data into a DataFrame. json_normalize flattens the data, meaning it can create a DataFrame from nested JSON data.\n",
    "df = json_normalize(data)\n",
    "\n",
    "# Print the first 5 rows of the DataFrame to see what the data looks like.\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame to a CSV file named \"initaldataframe.csv\" so you can see the progress. The argument index=False means that the DataFrame's index will not be saved in the CSV file.\n",
    "# While this step isn't necessary, it's a good idea to save your work as you go along and to check that the data looks correct.\n",
    "# If you are using VS Code, I highly recommend installing the Excel Viewer extension or Rainbow CSV extension so you can view the CSV file more easily.\n",
    "df.to_csv('initaldataframe.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data Up\n",
    "\n",
    "We will do some very simple cleanup to make dealing with our data easier. First, let's take out columns that we obviously won't need for our goals to make the data even more easy to analyze. Take a look at the initaldataframe.csv and determine what columns you think should be kept for our analysis. At this early stage it's usually a good idea to keep a column if in doubt. We can always trim it out later. Since we are keeping it simple we can hack and slash a bit more than usual. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'lang', 'entities.hashtags', 'entities.symbols', 'entities.user_mentions', 'entities.urls', 'metadata.iso_language_code', 'metadata.result_type', 'user.id', 'user.id_str', 'user.name', 'user.screen_name', 'user.location', 'user.description', 'user.url', 'user.entities.url.urls', 'user.entities.description.urls', 'user.protected', 'user.followers_count', 'user.friends_count', 'user.listed_count', 'user.created_at', 'user.favourites_count', 'user.utc_offset', 'user.time_zone', 'user.geo_enabled', 'user.verified', 'user.statuses_count', 'user.lang', 'user.contributors_enabled', 'user.is_translator', 'user.is_translation_enabled', 'user.profile_background_color', 'user.profile_background_image_url', 'user.profile_background_image_url_https', 'user.profile_background_tile', 'user.profile_image_url', 'user.profile_image_url_https', 'user.profile_banner_url', 'user.profile_link_color', 'user.profile_sidebar_border_color', 'user.profile_sidebar_fill_color', 'user.profile_text_color', 'user.profile_use_background_image', 'user.has_extended_profile', 'user.default_profile', 'user.default_profile_image', 'user.following', 'user.follow_request_sent', 'user.notifications', 'user.translator_type', 'user.withheld_in_countries', 'entities.media', 'extended_entities.media', 'retweeted_status.created_at', 'retweeted_status.id', 'retweeted_status.id_str', 'retweeted_status.full_text', 'retweeted_status.truncated', 'retweeted_status.display_text_range', 'retweeted_status.entities.hashtags', 'retweeted_status.entities.symbols', 'retweeted_status.entities.user_mentions', 'retweeted_status.entities.urls', 'retweeted_status.entities.media', 'retweeted_status.extended_entities.media', 'retweeted_status.metadata.iso_language_code', 'retweeted_status.metadata.result_type', 'retweeted_status.source', 'retweeted_status.in_reply_to_status_id', 'retweeted_status.in_reply_to_status_id_str', 'retweeted_status.in_reply_to_user_id', 'retweeted_status.in_reply_to_user_id_str', 'retweeted_status.in_reply_to_screen_name', 'retweeted_status.user.id', 'retweeted_status.user.id_str', 'retweeted_status.user.name', 'retweeted_status.user.screen_name', 'retweeted_status.user.location', 'retweeted_status.user.description', 'retweeted_status.user.url', 'retweeted_status.user.entities.description.urls', 'retweeted_status.user.protected', 'retweeted_status.user.followers_count', 'retweeted_status.user.friends_count', 'retweeted_status.user.listed_count', 'retweeted_status.user.created_at', 'retweeted_status.user.favourites_count', 'retweeted_status.user.utc_offset', 'retweeted_status.user.time_zone', 'retweeted_status.user.geo_enabled', 'retweeted_status.user.verified', 'retweeted_status.user.statuses_count', 'retweeted_status.user.lang', 'retweeted_status.user.contributors_enabled', 'retweeted_status.user.is_translator', 'retweeted_status.user.is_translation_enabled', 'retweeted_status.user.profile_background_color', 'retweeted_status.user.profile_background_image_url', 'retweeted_status.user.profile_background_image_url_https', 'retweeted_status.user.profile_background_tile', 'retweeted_status.user.profile_image_url', 'retweeted_status.user.profile_image_url_https', 'retweeted_status.user.profile_banner_url', 'retweeted_status.user.profile_link_color', 'retweeted_status.user.profile_sidebar_border_color', 'retweeted_status.user.profile_sidebar_fill_color', 'retweeted_status.user.profile_text_color', 'retweeted_status.user.profile_use_background_image', 'retweeted_status.user.has_extended_profile', 'retweeted_status.user.default_profile', 'retweeted_status.user.default_profile_image', 'retweeted_status.user.following', 'retweeted_status.user.follow_request_sent', 'retweeted_status.user.notifications', 'retweeted_status.user.translator_type', 'retweeted_status.user.withheld_in_countries', 'retweeted_status.geo', 'retweeted_status.coordinates', 'retweeted_status.place', 'retweeted_status.contributors', 'retweeted_status.is_quote_status', 'retweeted_status.retweet_count', 'retweeted_status.favorite_count', 'retweeted_status.favorited', 'retweeted_status.retweeted', 'retweeted_status.possibly_sensitive', 'retweeted_status.lang', 'retweeted_status.user.entities.url.urls', 'quoted_status_id', 'quoted_status_id_str', 'quoted_status.created_at', 'quoted_status.id', 'quoted_status.id_str', 'quoted_status.full_text', 'quoted_status.truncated', 'quoted_status.display_text_range', 'quoted_status.entities.hashtags', 'quoted_status.entities.symbols', 'quoted_status.entities.user_mentions', 'quoted_status.entities.urls', 'quoted_status.metadata.iso_language_code', 'quoted_status.metadata.result_type', 'quoted_status.source', 'quoted_status.in_reply_to_status_id', 'quoted_status.in_reply_to_status_id_str', 'quoted_status.in_reply_to_user_id', 'quoted_status.in_reply_to_user_id_str', 'quoted_status.in_reply_to_screen_name', 'quoted_status.user.id', 'quoted_status.user.id_str', 'quoted_status.user.name', 'quoted_status.user.screen_name', 'quoted_status.user.location', 'quoted_status.user.description', 'quoted_status.user.url', 'quoted_status.user.entities.url.urls', 'quoted_status.user.entities.description.urls', 'quoted_status.user.protected', 'quoted_status.user.followers_count', 'quoted_status.user.friends_count', 'quoted_status.user.listed_count', 'quoted_status.user.created_at', 'quoted_status.user.favourites_count', 'quoted_status.user.utc_offset', 'quoted_status.user.time_zone', 'quoted_status.user.geo_enabled', 'quoted_status.user.verified', 'quoted_status.user.statuses_count', 'quoted_status.user.lang', 'quoted_status.user.contributors_enabled', 'quoted_status.user.is_translator', 'quoted_status.user.is_translation_enabled', 'quoted_status.user.profile_background_color', 'quoted_status.user.profile_background_image_url', 'quoted_status.user.profile_background_image_url_https', 'quoted_status.user.profile_background_tile', 'quoted_status.user.profile_image_url', 'quoted_status.user.profile_image_url_https', 'quoted_status.user.profile_banner_url', 'quoted_status.user.profile_link_color', 'quoted_status.user.profile_sidebar_border_color', 'quoted_status.user.profile_sidebar_fill_color', 'quoted_status.user.profile_text_color', 'quoted_status.user.profile_use_background_image', 'quoted_status.user.has_extended_profile', 'quoted_status.user.default_profile', 'quoted_status.user.default_profile_image', 'quoted_status.user.following', 'quoted_status.user.follow_request_sent', 'quoted_status.user.notifications', 'quoted_status.user.translator_type', 'quoted_status.user.withheld_in_countries', 'quoted_status.geo', 'quoted_status.coordinates', 'quoted_status.place', 'quoted_status.contributors', 'quoted_status.is_quote_status', 'quoted_status.retweet_count', 'quoted_status.favorite_count', 'quoted_status.favorited', 'quoted_status.retweeted', 'quoted_status.lang', 'place.id', 'place.url', 'place.place_type', 'place.name', 'place.full_name', 'place.country_code', 'place.country', 'place.contained_within', 'place.bounding_box.type', 'place.bounding_box.coordinates', 'geo.type', 'geo.coordinates', 'coordinates.type', 'coordinates.coordinates', 'quoted_status.quoted_status_id', 'quoted_status.quoted_status_id_str', 'quoted_status.possibly_sensitive']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all column names\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# Print the list of column names\n",
    "print(all_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base column names you want to keep\n",
    "# copy and paste from the output above to make sure you get the column names correct\n",
    "base_columns_to_keep = ['created_at', 'id', 'full_text', 'in_reply_to_screen_name']\n",
    "\n",
    "# Add the metadata and user columns to the list since those are somewhat interesting to us\n",
    "columns_to_keep = base_columns_to_keep + [col for col in all_columns if col.startswith('metadata.') or col.startswith('user.')]\n",
    "\n",
    "# Keep only the desired columns in the DataFrame\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# print out the results to a csv file to check the results\n",
    "df.to_csv('limitedcolumns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at limitedcolumns.csv, it looks like we can safely remove some columns\n",
    "# Let's remove the columns that have no data in them\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# print out the results to a csv file to check the results\n",
    "df.to_csv('limitedcolumns_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's remove the columns that have the word \"url\" or \"color\" in them since they don't seem to be useful\n",
    "df = df[df.columns.drop(list(df.filter(regex='url|color')))]\n",
    "\n",
    "# print out the results to a csv file to check the results\n",
    "df.to_csv('limitedcolumns_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the withheld_in_countries column because it doesn't have usable values\n",
    "df.drop(columns=['user.withheld_in_countries'], inplace=True)\n",
    "\n",
    "# let's also change all the columns with boolean values to be 1 or 0 instead of True or False\n",
    "# this makes it easier to work with the data later\n",
    "df = df.replace({True: 1, False: 0})\n",
    "\n",
    "# print out the results to a csv file to check the results\n",
    "df.to_csv('limitedcolumns_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have trimmed the data down to 27 columns now and the data looks much more manageable\n",
    "# a couple of more cleanup items and we will be ready to start analyzing the data\n",
    "# let's convert all the column names to lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# finally, let's replace any \".\" in the column names with \"_\" to be consistent\n",
    "df.columns = df.columns.str.replace('.', '_')\n",
    "\n",
    "# print out the results to a csv file to check the results\n",
    "df.to_csv('limitedcolumns_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment and Emotion Analysis\n",
    "Now on to the first set of analysis: \n",
    "For each Tweet, we need to find out the main sentiment (Positive, Neutral, Negative) and the main emotion (Joy, Surprise, Neutral, Sadness, Mistrust, and Disgust)\n",
    "We will want to return, both, the predicted sentiment and emotion, as well as the score (ranging for -1 (negative) to 1 (positive) for Sentiment and 0 (disgust) to 1 (joy) for emotion).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to take in our dataframe and return a new dataframe \n",
    "# with the columns for sentiment and emotion added to each tweet\n",
    "def sentiment_emotion_analysis(df):\n",
    "    # Initialize empty lists to hold sentiment and emotion data\n",
    "    sentiments = []\n",
    "    emotions = []\n",
    "\n",
    "    # Loop through every tweet in the dataframe's 'full_text' column using Few-Shot Prompting\n",
    "    for tweet in df['full_text']:\n",
    "        # Define the prompt for GPT-3 to analyze sentiment and emotion\n",
    "        # note we use the f-string (formatted string) to insert the tweet into the prompt\n",
    "        # we should be doing few-shot chain-of-thought prompting but we don't have time\n",
    "        myprompt=f\"\"\"\n",
    "        Analyze the sentiment of the following tweet and provide a sentiment rating from -1 (completely negative) to 1 (completely positive) and all values in between. If the sentiment is neutral, provide a score of 0. Also, provide an emotion associated with the tweet, choosing from Joy, Surprise, Neutral, Sadness, Mistrust, and Disgust respectively. Rate the emotions from complete joy(1) down to complete disgust(0), assigning values to the emotions as follows: Joy = 1, Surprise = 0.80, Neutral = 0.60, Sadness = 0.40, Mistrust = 0.20, and Disgust = 0. \n",
    "\n",
    "        Here are some examples:\n",
    "\n",
    "        Tweet: Who is making these decisions @SamsClub Do you hate your employees?? Are you kidding?! The @weatherchannel is here. Put it on tv, pay attention to what they are saying, and tell your employees to stay home! #HurricaneIdalia\n",
    "        Sentiment: -0.8\n",
    "        Emotion: Disgust\n",
    "\n",
    "        Tweet: Sams club has frozen grill cheese sandwiches ü§îü§îsheer genius üòçüòç\n",
    "        Sentiment: 0.8\n",
    "        Emotion: Joy\n",
    "\n",
    "        Tweet: signed up for a sams club membership. this feel wayyy too grown for me\n",
    "        Sentiment: 0.2\n",
    "        Emotion: Sadness\n",
    "\n",
    "        Tweet: I really need to go sams club\n",
    "        sentiment: 0.0\n",
    "        Emotion: Neutral\n",
    "\n",
    "        Tweet: moms coming home@with sams club pizza!!\n",
    "        Sentiment: 0.9\n",
    "        Emotion: Surprise\n",
    "\n",
    "        Provide the sentiment and emotion ratings in the following JSON format:\n",
    "        {{\"Sentiment\": \"<sentiment>\", \"Sentiment_Value\": <sentiment_value>, \"Emotion\": \"<emotion>\", \"Emotion_Value\": <emotion_value>}}\n",
    "\n",
    "        What is the sentiment and emotion of this tweet:\n",
    "        \"{tweet}\"\n",
    "\n",
    "        Answer: {{\"Sentiment\": \"<sentiment>\", \"Sentiment_Value\": <sentiment_value>, \"Emotion\": \"<emotion>\", \"Emotion_Value\": <emotion_value>}}\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Create an API call to OpenAI using the specified model, prompt, and parameters\n",
    "        # With more time we can play with temperature and top_p to see if we can get better results\n",
    "        response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",  # The model used for prediction\n",
    "            prompt=myprompt,  # The prompt defined above\n",
    "            temperature=0,  # Controls the randomness of the model's output\n",
    "            max_tokens=100,  # The maximum number of tokens in the output\n",
    "            top_p=0.50,  # The nucleus sampling parameter, which controls the randomness of token generation\n",
    "            frequency_penalty=0.0,  # Penalizes the frequency of tokens in the output\n",
    "            presence_penalty=0.0  # Penalizes the new tokens in the output\n",
    "        )\n",
    "\n",
    "        # Get the response text and remove the initial \"Answer: \"\n",
    "        response_text = response['choices'][0]['text'].strip().replace(\"Answer: \", \"\")\n",
    "\n",
    "        # Process the response text\n",
    "        try:\n",
    "            # Load the response into a JSON object\n",
    "            response_json = json.loads(response_text)\n",
    "\n",
    "            # Extract the sentiment and emotion data from the JSON object\n",
    "            sentiment = response_json[\"Sentiment\"]\n",
    "            sentiment_value = response_json[\"Sentiment_Value\"]\n",
    "            emotion = response_json[\"Emotion\"]\n",
    "            emotion_value = response_json[\"Emotion_Value\"]\n",
    "\n",
    "            # Append the sentiment and emotion data to the corresponding lists\n",
    "            sentiments.append((sentiment, sentiment_value))\n",
    "            emotions.append((emotion, emotion_value))\n",
    "\n",
    "        # If there's an error processing the response text, append \"Error\" and 0 to the data\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing tweet: {tweet}\")\n",
    "            print(f\"Response from API: {response_text}\")\n",
    "            sentiments.append((\"Error\", 0))\n",
    "            emotions.append((\"Error\", 0))\n",
    "\n",
    "    # Add the sentiment and emotion data to the dataframe as new columns\n",
    "    df['sentiment'], df['sentiment_value'] = zip(*sentiments)\n",
    "    df['emotion'], df['emotion_value'] = zip(*emotions)\n",
    "\n",
    "    # Return the updated dataframe\n",
    "    return df\n",
    "\n",
    "# Call the function and update the dataframe\n",
    "df = sentiment_emotion_analysis(df)\n",
    "# Save the updated dataframe as a CSV file\n",
    "df.to_csv('sentimentemotion.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
